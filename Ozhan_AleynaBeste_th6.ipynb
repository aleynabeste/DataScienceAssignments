{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ozhan_AleynaBeste_th6.ipynb","provenance":[{"file_id":"1gHlqe-gX48HrVOi6l7sqPYHVxT5m_4-b","timestamp":1589296733318}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gzA5v_dYmrKa","colab_type":"text"},"source":["# Take-Home Exam 6: General Review\n","\n","*In this take-home exam, you are going to solve questions regarding machine learning concepts.*\n","\n","**Submission Instructions**\n","\n","---\n","Copy this assignment to your Drive. <font color = 'red'> `File` --> `Save a copy in Drive`</font>. Rename it as <font color = 'green'>`Lastname_Firstname_th6`</font>.\n","\n","Write your solutions in the cells  marked <font color = 'green'>`# your code`</font>.\n","\n","When you're done please submit your solutions as an <font color=\"red\">`.ipynb`</font> file. To do so:\n","\n","\n","1.  Click on <font color=\"red\">`File`</font>  at the top left on the Colab screen, then click on <font color = 'red'>`Download .ipynb`</font>.\n","2.   Then submit the <font color=\"red\">`.ipynb`</font> version of your work on SUCourse.\n","\n","\n","For any question, you may send an email to the TAs and LAs.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"3PPXszHTQ306","colab_type":"text"},"source":["**IMPORTANT**\n","\n","In order to complete the true/false and multi-choice questions, please use markdown utilities. For instance, below you have a set of options \n","\n","- [ ] option A\n","- [x] option B\n","- [ ] option C\n","\n","If you **double click** on this cell, you will see that a ticked option is represented with \\[x\\] statement. Please utilize the same selection mechanism in the questions below."]},{"cell_type":"markdown","metadata":{"id":"HkpA2ho_LNxB","colab_type":"text"},"source":["## ML Concepts"]},{"cell_type":"markdown","metadata":{"id":"KFXTYe2DL7KI","colab_type":"text"},"source":["### Q1\n","\n","What is the difference between classification and regression? Assume that you are performing price prediction. Which is the appropriate method? Explain your reasoning.\n","\n","\n","**Classification and regression are both subgroups of supervised machine learning, that is they both try to learn a mapping function(f(x)=y) with given prior knowledge of observations(x) and corresponding outputs(y). In the case of regression, the output values (y )of the mapping function are continuous real variables such as integer, floats etc. When we want to predict the price, we use regression because price values are continuous. On the other hand, in the case of classification, outtput values (y) of the mapping function are discrete/categorial variables. For example if you want to predict the neighborhood of the house, you use classification since neighborhood names are discrete, we can categorize them. Please note that if you want to predict price like as in categories \"low price\"-\"high price\", then you use classification. In other words, classification predicts label, regression predicts quantity. When we want to evaluate the prediction performance, we use acuracy score for classification in terms of percentage. Accuracy score indicates the probability of function to map the value correctly. When we wanto to evaluate the prediction in case of regression, we use root mean squared error, not accuracy.**\n"]},{"cell_type":"markdown","metadata":{"id":"j4etmk03T6Z1","colab_type":"text"},"source":["### Q2\n","\n","What is overfitting? Explain in terms of its relationship with model complexity and its implications.\n","\n","\n","**When we create a ML model and train it, we basically make our mapping function learn concepts from the training data. Afterwards, when we try to apply this model to another unseen dataset, the performance efficieny of the model is how well our model applies its learned concepts to unseen data.We want our model to be efficient and works well on every unseen dataset(from the same domain as training data of course). Therefore one might think that the more model learns and trains, the more accurate it gets. It is a wrong perception and a route cause of overfitting. Overfitting happens when model is trained so much on train data set that it starts to memorize the traning data concepts, features or patterns. Model learns every tiny detail even the noises, it starts to think(!) that those noises and irrelevant concepts(feature patterns that dont obey to  the overall trajectory) are actually real concepts or features.Model fits itself so closely to those details and makes itself very complex. This situation leads to failure when it comes to applying this model to another dataset because model tries to find exact same noises, fluctuations and concepts that it is learned from training data, it tries to find the same complexity. It can't find of course, because noises, irrelevant datas are all different from set to set.  In other words model gets too complicated, it learns by heart the training data by considering every tiny detail in training data. Afterwars when it comes to perform on another unseen data set, new data cannot fit into this detailed complex model, resulting in a decrease in accuracy. Metaporically speaking, suppose you make a custom design dress according to body size of a specific person, another person cannot fit into that dress easily but this dress will look great on the owner of dress. But if you make a large dress without details of body sizes etc, someone else can fit in also.Overfitted model can be understood by looking at the accuracy values on test and train data. If there is a significant difference: high accuracy value on train data, low accuracy on test data, then there is overfitting. Complex model memorizes train data so much that it cannot predict well on another different data.**\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f6kMHqNwHRiu","colab_type":"text"},"source":["### Q3\n","\n","Why do we need cross-validation? \n","\n","**While creating our models, we split our data into two groups namely test and train sets (generally %80 train). Then we train our model on train set, our model learns from this set. Also we know that more data you have, more clever(!) your model gets as it practices with more data and sees different examples. By providing large training set, you basically provide the chance of seeing all possible examples so that model learns them and become a more accurate, well performing model. However, by splitting the data, there is a chance that you don't include important examples in the train set and they go to the test set. In this case, your model misses the chance of learning them. You will never make sure that all important examples are in train set.Accuracy result depends on that specific split. Therefore, cross validation is used to solve this problem. In cross validation, you randomly splits data into n many sub groups and set one of them as test data, and set all the rest as train data. After, you make n many iterations in which you perform split differently and in each iteration you train your model with a different train set. Hence get the chance of training your model with different train sets and decrease the risk of losing important examples that you wish your model learn.**"]},{"cell_type":"markdown","metadata":{"id":"tkaD4GWbAF0H","colab_type":"text"},"source":["## kNN"]},{"cell_type":"markdown","metadata":{"id":"MQayVyetNhxq","colab_type":"text"},"source":["### Q1\n","\n","A kNN model with k set to 1 would obtain 100% accuracy result on the train data.\n","\n","- [x] True\n","- [ ] False"]},{"cell_type":"markdown","metadata":{"id":"RX8an6jJOlZJ","colab_type":"text"},"source":["### Q2\n","\n","Given a training dataset with 100 observations, we built a kNN model with k set to 100. In the training data, 30% of the observations are of class `A`, 10% are of class `B` and the remaining 60% are of class `C`. \n","\n","Now, we have a test dataset in which 5% are of class `A`, 55% are of class `B` and remaining 40% are of class `C`. If we deploy the trained model to predict the test dataset, what would be the resulting accuracy score?\n","\n","- [ ] 50%\n","- [x] 40%\n","- [ ] 55%"]},{"cell_type":"markdown","metadata":{"id":"YsTpi3s9AHqR","colab_type":"text"},"source":["## Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"jBcChfpoVuhj","colab_type":"text"},"source":["### Q1\n","\n","Suppose we have the training data below, where A, B and C are binary attributes and y is the target attribute.\n","\n","|A|B|C|y|\n","|:---:|:---:|:---:|:---:|\n","|0|1|0|yes|\n","|1|0|1|yes|\n","|0|0|0|no|\n","|1|0|1|no|\n","|0|1|1|no|\n","|1|1|0|yes|\n","\n","Assume we have a decision tree model trained with this model. Is it possible for this model to obtain 100% accuracy score on the training data? Explain your reasoning.\n","\n","**No, because we see in the data that row2 and row4 have same features (A,B,C=1,0,1)but they have different labels, one is yes one is no. While training, one of these label is learned hence other one will be automatically labeled false.In this case, accuracy can't be 100% because not all labels will be correct**"]},{"cell_type":"markdown","metadata":{"id":"_VhPe7Iwczb8","colab_type":"text"},"source":["### Q2\n","\n","Suppose we have the training data below, where A, B and C are binary attributes and y is the target attribute.\n","\n","|A|B|C|y|\n","|:---:|:---:|:---:|:---:|\n","|0|1|0|yes|\n","|1|0|1|yes|\n","|0|0|0|no|\n","|1|0|1|no|\n","|0|1|1|no|\n","|1|1|0|yes|\n","|0|1|1|yes|\n","\n","Which attribute above would yield the highest information gain for the root node in the decision tree?\n","\n","- [ ] A\n","- [x] B\n","- [ ] C"]},{"cell_type":"markdown","metadata":{"id":"qrUqwwp1Qn6u","colab_type":"text"},"source":["### Q3\n","\n","A decision tree algorithm iteratively evaluates available features in the data to create the branches. What would be the effect of adding a new feature to the dataset on the predictive performance of a model?\n","\n","- [ ] Increases the performance\n","- [ ] Decreases the performance\n","- [x] There is no enough information"]},{"cell_type":"markdown","metadata":{"id":"2GVphvifLSGq","colab_type":"text"},"source":["## Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"y6oUnpswLToT","colab_type":"text"},"source":["### Q1\n","\n","Why is Naive Bayes algorithm considered as `naive`? Explain briefly and discuss its implications in terms of its effect on the computation.\n","\n","**Naive Bayes is considered as ‘naive’ because it assumes that features of a measurement are independent of each other. Since this is not the case in reality, its called naive**.  \n"]},{"cell_type":"markdown","metadata":{"id":"PyloVcBXW9o4","colab_type":"text"},"source":["### Q2\n","\n","Suppose that we have the data below, where `Stolen` is the  target attribute that consists of binary values (yes or no).\n","\n","|Color|Type|Origin|Stolen|\n","|:---:|:---:|:---:|:---:|\n","|red|sports|domestic|yes|\n","|red|sports|domestic|no|\n","|red|sports|domestic|yes|\n","|yellow|sports|domestic|no|\n","|yellow|sports|imported|yes|\n","|yellow|suv|imported|no|\n","|yellow|suv|imported|yes|\n","|yellow|suv|domestic|no|\n","|red|suv|imported|no|\n","|red|sports|imported|yes|\n","\n","You have a naive bayes model and a new test instance with attributes `color=red`, `type=suv` and `origin=imported`. What would be the predicted label by the naive bayes model for the given test instance? *Write down your calculations step by step.*\n","\n","There is no need to use latex in this question. You may show your calculations as P(stolen=yes|color=red....) = (...)/(...) etc.\n","\n","\n","\n","P(stolen=yes)= 5/10=0.5\n","\n","P(stolen=no)= 5/10=0.5\n","\n","P(red | yes) = 3/5 = 0.6\n","\n","P(red | no)= 2/5 = 0.4\n","\n","P(suv | yes) = 1/5 = 0.2\n","\n","P(suv | no) = 3/5 = 0.6\n","\n","P(imported | yes) =  3/5 = 0.6\n","\n","P(imported | no) = 2/5 =0.4\n","\n","\n","P(yes|X) = P(red | yes) * P(imported | yes) * P(suv | yes) * P(yes) = 0.6 * 0.2 * 0.6 * 0.5 = 0.036\n","\n","\n","P(no|X) = P(red | no) * P(imported | no) * P(suv | no) * P(no) = 0.4* 0.6 * 0.4* 0.5 = 0.048\n","\n","P(yes|X)= (0.036/(0.036+0.048))= 0.42\n","\n","P(no|X) = (0.048 /(0.036+0.048))=0.58\n","\n","P(no|X) is higher hence the label is going to be NO. \n"]}]}